version: '3.8'

services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlflow/mlruns
      - ./mlartifacts:/mlflow/mlartifacts
    command: >
      mlflow server 
      --host 0.0.0.0 
      --port 5000
      --backend-store-uri sqlite:///mlflow/mlruns/mlflow.db
      --default-artifact-root file:///mlflow/mlartifacts
      --allowed-hosts mlflow,mlflow:5000,localhost,localhost:5000,127.0.0.1,127.0.0.1:5000,0.0.0.0
    # start server with specified backend and artifact locations and allow access from specified hosts
    networks:
      - rag-network

  rag-app:
    build: .
    ports:
      - "8000:8000"   # Keep only if you run a server. Remove if not needed.
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      MLFLOW_TRACKING_URI: http://mlflow:5000
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    depends_on:
      - mlflow
    networks:
      - rag-network

networks:
  rag-network:
    driver: bridge
